{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import sys, json\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries \n",
    "    Inspired by: http://dlab.berkeley.edu/blog/scraping-new-york-times-articles-python-tutorial\n",
    "    '''\n",
    "    news = []\n",
    "    for i in articles['response']['docs']:\n",
    "        dic = {}\n",
    "        dic['id'] = i['_id']\n",
    "        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n",
    "        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n",
    "        dic['section'] = i['section_name']\n",
    "        if i['snippet'] is not None:\n",
    "            dic['snippet'] = i['snippet'].encode(\"utf8\")\n",
    "        dic['source'] = i['source']\n",
    "        dic['type'] = i['type_of_material']\n",
    "        dic['url'] = i['web_url']\n",
    "        dic['word_count'] = i['word_count']\n",
    "        # locations\n",
    "        locations = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'glocations' in i['keywords'][x]['name']:\n",
    "                locations.append(i['keywords'][x]['value'])\n",
    "        dic['locations'] = locations\n",
    "\n",
    "        # subject\n",
    "        subjects = []\n",
    "        for x in range(0,len(i['keywords'])):\n",
    "            if 'subject' in i['keywords'][x]['name']:\n",
    "                subjects.append(i['keywords'][x]['value'])\n",
    "        dic['subjects'] = subjects   \n",
    "        news.append(dic)\n",
    "    return(news) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def articles_fetch(beginDate, endDate):\n",
    "    '''\n",
    "    Inputs beginDate and endDate (YYYYMMDD), returns all articles with query terms: economy, American, market\n",
    "    '''\n",
    "\n",
    "    #Modify each aspect of API Request URL\n",
    "    apiUrl='https://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "    key = 'api-key=12e8e79c0c184049bd19328c04daf1fb' #your API key\n",
    "    query='&q=economy+American+market'  # set the query word here (AND, not OR), terms delimited by +\n",
    "\n",
    "    apiDate = '&begin_date=' + str(beginDate) + '&end_date=' + str(endDate)\n",
    "    sort = \"&sort=oldest\"\n",
    "    page = '&page=0'\n",
    "\n",
    "    #combine and create URL\n",
    "    link = [apiUrl, key, query, apiDate, sort, page]\n",
    "    reqUrl = ''.join(link)\n",
    "    \n",
    "    #print URL to test\n",
    "    print reqUrl\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "    #open url and load\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    \n",
    "    req = urllib2.Request(reqUrl, headers=hdr)\n",
    "        \n",
    "    try:\n",
    "        page = urllib2.urlopen(req)\n",
    "    except urllib2.HTTPError, e:\n",
    "        time.sleep(1)\n",
    "        page = urllib2.urlopen(req)\n",
    "  \n",
    "        \n",
    "    response = page.read()\n",
    "    test = json.loads(response)\n",
    "    \n",
    "    #number of articles\n",
    "    num_results = test['response']['meta']['hits']\n",
    "    \n",
    "    #number of pages to go through\n",
    "    num_pages = num_results/10\n",
    "\n",
    "\n",
    "    #keep everything in all_articles    \n",
    "    all_articles = []\n",
    "    test = parse_articles(test)\n",
    "    all_articles = all_articles + test\n",
    "\n",
    "    for i in range(1, num_pages + 1):\n",
    "        #sleep for 1 sec between each request to prevent API Limit\n",
    "        time.sleep(1)\n",
    "        page = '&page=' + str(i)\n",
    "        \n",
    "        link = [apiUrl, key, query, apiDate, sort, page]\n",
    "        reqUrl = ''.join(link)\n",
    "\n",
    "        print i\n",
    "       \n",
    "        req = urllib2.Request(reqUrl, headers=hdr)\n",
    "        \n",
    "        try:\n",
    "            page = urllib2.urlopen(req)\n",
    "        except urllib2.HTTPError, e:\n",
    "            continue\n",
    "            print e.fp.read()\n",
    "        \n",
    "        response = page.read()\n",
    "\n",
    "        test = json.loads(response)\n",
    "        try:\n",
    "            test = parse_articles(test)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        all_articles = all_articles + test\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def year_fetch(startYear, endYear):\n",
    "    '''\n",
    "    Accepts starting and finishing year as inputs, saves CSV of articles \n",
    "    '''\n",
    "    for i in range(startYear, endYear + 1):\n",
    "        time.sleep(1)\n",
    "        print i\n",
    "        fetched = articles_fetch(str(i) +\"0101\", str(i) + \"1231\")\n",
    "        keys = fetched[1].keys()\n",
    "        \n",
    "        with open('economy-mentions_' + str(i) + '.csv', 'wb') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(fetched)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startYr = input(\"Starting year? \\n\")\n",
    "endYr = input(\"Ending year? \\n\")\n",
    "\n",
    "#insert startyear, secondyear\n",
    "year_fetch(startYr, endYr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
